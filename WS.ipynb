{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various tools to scrape a web page. Selenium package in python is one of them. Its advantage and the reason i decided to use this particular package is that Selenium allows to scrape information from rendered by Javascript websites. Most of websites use Javascript which runs on a web page after it is loaded into a browser to create additional HTML elements.\n",
    "\n",
    "As a result fetching original HTML code wouldn't allow me to navigate web page and capture information from it. Beautiful Soup is another python package which can only retrieve original HTML. Selenium on the other hand is capable of working with rendered HTML code. \"Ctrl+Shift+I\" is used to see HTML code of a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing python libraries\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have previously installed ChromeDriver to use Selenium with Chrome browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will start chrome browser and open indeed.com\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.indeed.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In chrome browser on Debian \"Ctrl+Shift+I\" allows to inspect HTML elements of the page.\n",
    "\n",
    "And this is how it looks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"screenshot1.png\" align=\"left\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](screenshot1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find html code for any element on the page i selected arrow at the top in a red circle and pointed at \"Company Reviews\" in this case. The html code got highlighted in blue on the right. I can see that the element can be identified by tag \"a\" and href attribute \"/companies\". Now i can work with this element. The below code allows to find the element and click on it. Let's scrape a few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on \"company reviews\"\n",
    "driver.find_element_by_xpath('//a[@href=\"/companies\"]').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_element_by_xpath function searches for web element based on XML path. In the code above i use tag name \"a\" and attribute \"href\" value \"/companies\". Here is a good tutorial on XML path: https://www.guru99.com/xpath-selenium.html. \n",
    "\n",
    "I am going to collect information from \"popular companies\" section of the web site: company name and number of reviews. Then identify a company with highest number of reviews and capture those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find \"popular companies\" area on the page\n",
    "table = driver.find_element_by_xpath(\"//div[text() = 'Popular Companies']/following-sibling::div\")\n",
    "# Capture all elements associated with companies from the area above\n",
    "companies = table.find_elements_by_xpath(\"//div[@class='cmp-PopularCompaniesWidget-companyName']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"screenshot2.png\" align=\"left\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using similar method in the above code i identified area of the page for \"popular companies\" section and then elements for each item (company) in that area. The XML paths above use tag \"div\" and 2 attributes: text and class in this case. Following-sibling::div means to look for next element with tag \"div\" at the same level. That's why it's called \"sibling\".\n",
    "\n",
    "The function below will capture company name and number of reviews and save this information in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_companies_info(companies):\n",
    "    comp_dict = {}\n",
    "    for i in range(len(companies)):\n",
    "        # capture company name\n",
    "        name = companies[i].find_element_by_tag_name(\"a\").text\n",
    "        # capture number of reviews\n",
    "        reviews = companies[i].find_element_by_xpath(\"./following-sibling::a\").text\n",
    "        comp_dict[i] = {\"name\":name, \"reviews\":reviews}\n",
    "    return comp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Home Depot</td>\n",
       "      <td>43,693 reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kmart</td>\n",
       "      <td>18,477 reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kroger Stores</td>\n",
       "      <td>25,744 reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bank of America</td>\n",
       "      <td>26,470 reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sears</td>\n",
       "      <td>25,854 reviews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name         reviews\n",
       "0   The Home Depot  43,693 reviews\n",
       "1            Kmart  18,477 reviews\n",
       "2    Kroger Stores  25,744 reviews\n",
       "3  Bank of America  26,470 reviews\n",
       "4            Sears  25,854 reviews"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put company name and number of reviews in a dataframe\n",
    "comp_dict = get_companies_info(companies)\n",
    "comp_df = pd.DataFrame().from_dict(comp_dict, orient='index')\n",
    "comp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below finds a company with the highest number of reviews in the dataframe then identifies element that corresponds to this company on a web page and navigates to this company page to collect its reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve numeric values from character \"reviews\" field\n",
    "comp_df[\"reviews_n\"] = comp_df[\"reviews\"].apply(lambda x: int(x.split()[0].replace(',', '')))\n",
    "# Find company with highest nubmer of reviews\n",
    "max_reviews = comp_df.iloc[comp_df['reviews_n'].idxmax()]['reviews']\n",
    "# Find HTML element on the page with corresponding number of reviews and click on it\n",
    "table.find_element_by_xpath('.//div[text() = \"%s\"]' % max_reviews).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Home Depot has the highest number of reviews so let' scrape those. But first i am going to define a few functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>reviews</th>\n",
       "      <th>reviews_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Home Depot</td>\n",
       "      <td>43,693 reviews</td>\n",
       "      <td>43693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name         reviews  reviews_n\n",
       "0  The Home Depot  43,693 reviews      43693"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_df.loc[comp_df['reviews'] == max_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple pages with reviews for each company. This function clicks on \"next page\" button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function tries to locate \"next page\" button. If it can't find the button it means this is the last page.\n",
    "# It's not going to throw an exception but return 0 instead \n",
    "def next_page():\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//a[@data-tn-element = \"next-page\"]').click()\n",
    "    except:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML path allows to find all elements representing individual text reviews on a page and capture text and ratings information. The following function does exactly that on a particular page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture all text company reviews along with ratings from a particular page and save them in a dictionary\n",
    "def grab_one_page():\n",
    "    one_page = driver.find_elements_by_xpath('//div[@id = \"cmp-content\"]/div[@class = \"cmp-review-container\"]')\n",
    "    reviews_dict = {}\n",
    "    for i in range(len(one_page)):\n",
    "        review_text = one_page[i].find_element_by_class_name(\"cmp-review-text\").text\n",
    "        review_rating = one_page[i].find_element_by_class_name(\"cmp-ratingNumber\").text\n",
    "        reviews_dict[i] = {\"review_text\":review_text, \"review_rating\":review_rating}\n",
    "    return reviews_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save dataframe with reviews information on a disk\n",
    "def save_results(reviews_df):\n",
    "    with open('results.pickle', 'wb') as f:\n",
    "        pickle.dump(reviews_df, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function checks whether title of the page has changed. I found that any problems with page download change the title. This way i can identify such issue and refresh the page before continuing with scraping information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy title of the page\n",
    "title = driver.title\n",
    "\n",
    "# Check whether title of the page changed and try to refresh the page a few times\n",
    "def check_page_load():\n",
    "    for i in range(3):\n",
    "        if driver.title == title:\n",
    "            return 1\n",
    "        else:\n",
    "            print(\"Reloading a page...\")\n",
    "            driver.refresh()\n",
    "            sleep(10)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function navigate through pages with reviews, collects and saves them. It uses the functions already defined above.\n",
    "\n",
    "While scraping information from indeed.com i am going to wait between 3 and 6 seconds before moving to the next reviews page. This is to make sure i am not hitting the server too frequently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will scrape reviews untill it reaches number of pages requested by user to scrape or number of reviews\n",
    "def scrape_reviews(max_number_pages, max_reviews, reviews_df):\n",
    "    for i in range(max_number_pages):\n",
    "        print(\"Page %i is being scraped\" %(i+1))\n",
    "        reviews_dict = grab_one_page()\n",
    "        \n",
    "        # save reviews in a dataframe\n",
    "        reviews_df = reviews_df.append(pd.DataFrame.from_dict(reviews_dict, orient='index'), ignore_index=True)\n",
    "        print(\"%i reviews have been collected so far\\n\" %(len(reviews_df)))\n",
    "        \n",
    "        # wait between 3 and 6 seconds to proceed\n",
    "        sleep(random.randint(3,6))\n",
    "        \n",
    "        # save dataframe on a disk every 10 pages\n",
    "        if i%10 == 0:\n",
    "            save_results(reviews_df)\n",
    "        \n",
    "        # in case there are no more pages with reviews left, save results and quit\n",
    "        if next_page() == 0:\n",
    "            save_results(reviews_df)\n",
    "            print(\"No more reviws, %i reviews were successfully captured\" %(len(reviews_df)))\n",
    "            return reviews_df\n",
    "        \n",
    "        # if number of reviews requested by user was captured, save results and quit\n",
    "        if len(reviews_df) > max_reviews:\n",
    "            save_results(reviews_df)\n",
    "            print(\"%i reviews captured is greater than number of reviews requested\" %(len(reviews_df)))\n",
    "            return reviews_df\n",
    "        \n",
    "        # if web page failed to load after trying to refresh it, save results and quit\n",
    "        if check_page_load() == 0:\n",
    "            save_results(reviews_df)\n",
    "            print(\"Page %i failed to load - %i reviews were captured\" %(i+2, len(reviews_df)))\n",
    "            return reviews_df\n",
    "    \n",
    "    # in case number of pages requested by user was captured, save results and quit\n",
    "    else:\n",
    "        save_results(reviews_df)\n",
    "        print(\"%i pages were successfully scraped as requested\" %(i+1))\n",
    "        return reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ready to start scraping data. I am going to collect 1000 reviews on 50 pages - there are 20 reviews per page. This will be enough to show how it works and i can collect more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame(columns=['review_text', 'review_rating'])\n",
    "reviews_df = scrape_reviews(50, 1000, reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the dataframe saved on a disk is the same as original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "with open('results.pickle', 'rb') as f:\n",
    "     reviews_df2 = pickle.load(f)\n",
    "\n",
    "print(reviews_df.equals(reviews_df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 reviews with ratings information can be a good start for a machine learning project, for example to train NLP model to predict review rating based on review text. This number can be increased by changing the function parameters or running the code a few times to avoid unnecessary load on the website."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
